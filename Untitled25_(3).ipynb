{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRUGZeP_HZ25",
        "outputId": "5b7ecff4-35f5-4e63-bea5-341b41e81213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated runtime:\n",
            "- Data preparation: ~5-10 seconds\n",
            "- Training (50 episodes): ~100-150 seconds\n",
            "- Total estimated time: ~160 seconds\n",
            "\n",
            "Starting training pipeline...\n",
            "\n",
            "Device being used: CUDA\n",
            "Loading and preparing data...\n",
            "Data preparation completed in 0.04 seconds\n",
            "Final dataset shape: (6197, 8)\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Episodes:   0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-1-2c89b69208e2>:100: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  states = torch.FloatTensor([i[0] for i in minibatch]).to(self.device)\n",
            "Training Episodes: 100%|██████████| 50/50 [14:22<00:00, 17.25s/it, Reward=-649.79, Epsilon=0.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed in 862.63 seconds\n",
            "Average time per episode: 17.25 seconds\n",
            "Best reward achieved: -149.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        # Simplified architecture for faster training\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class DataCenterEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.reset()\n",
        "        self.action_space = 5\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.total_energy = 0\n",
        "        self.temperature_violations = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "        return np.array([\n",
        "            current_data['meter_reading'],\n",
        "            current_data['temperature'],\n",
        "            current_data['humidity'],\n",
        "            current_data['hour'],\n",
        "            current_data['day'],\n",
        "            self.total_energy,\n",
        "            self.temperature_violations\n",
        "        ])\n",
        "\n",
        "    def step(self, action):\n",
        "        cooling_adjustment = (action - 2) * 0.1\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "        base_energy = current_data['meter_reading']\n",
        "        new_energy = base_energy * (1 + cooling_adjustment)\n",
        "\n",
        "        energy_savings = max(0, base_energy - new_energy)\n",
        "        temperature_penalty = -50 if current_data['temperature'] > 0.7 and action < 2 else 0\n",
        "\n",
        "        reward = energy_savings * 0.1 + temperature_penalty\n",
        "\n",
        "        self.total_energy += new_energy\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        return self._get_state(), reward, done\n",
        "\n",
        "class DRLAgent:\n",
        "    def __init__(self, state_size, action_size, batch_size=32):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=1000)  # Reduced memory size\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = DQN(state_size, action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            return np.argmax(self.model(state).cpu().numpy())\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = torch.FloatTensor([i[0] for i in minibatch]).to(self.device)\n",
        "        actions = torch.LongTensor([i[1] for i in minibatch]).to(self.device)\n",
        "        rewards = torch.FloatTensor([i[2] for i in minibatch]).to(self.device)\n",
        "        next_states = torch.FloatTensor([i[3] for i in minibatch]).to(self.device)\n",
        "        dones = torch.FloatTensor([i[4] for i in minibatch]).to(self.device)\n",
        "\n",
        "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q = self.model(next_states).detach().max(1)[0]\n",
        "        target_q = rewards + (self.gamma * next_q * (1 - dones))\n",
        "\n",
        "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "def prepare_data(sample_size=10000):\n",
        "    print(\"Loading and preparing data...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load only the first n rows for faster processing\n",
        "    train_df = pd.read_csv('train.csv', nrows=sample_size)\n",
        "\n",
        "    # Basic preprocessing\n",
        "    train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
        "    train_df['hour'] = train_df['timestamp'].dt.hour\n",
        "    train_df['day'] = train_df['timestamp'].dt.dayofweek\n",
        "\n",
        "    # Filter for electricity meters\n",
        "    train_df = train_df[train_df['meter'] == 0].reset_index(drop=True)\n",
        "\n",
        "    # Synthetic features\n",
        "    train_df['temperature'] = 20 + 5 * np.sin(train_df['hour'] * 2 * np.pi / 24)\n",
        "    train_df['humidity'] = 50 + 20 * np.sin(train_df['hour'] * 2 * np.pi / 24)\n",
        "\n",
        "    # Normalize\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_columns = ['meter_reading', 'temperature', 'humidity', 'hour', 'day']\n",
        "    train_df[numerical_columns] = scaler.fit_transform(train_df[numerical_columns])\n",
        "\n",
        "    print(f\"Data preparation completed in {time.time() - start_time:.2f} seconds\")\n",
        "    print(f\"Final dataset shape: {train_df.shape}\")\n",
        "\n",
        "    return train_df\n",
        "\n",
        "def train_model(episodes=50, sample_size=10000):\n",
        "    print(f\"\\nDevice being used: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "    # Prepare data\n",
        "    data = prepare_data(sample_size)\n",
        "\n",
        "    # Initialize environment and agent\n",
        "    env = DataCenterEnvironment(data)\n",
        "    state_size = 7\n",
        "    action_size = 5\n",
        "    agent = DRLAgent(state_size, action_size)\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    start_time = time.time()\n",
        "    best_reward = float('-inf')\n",
        "\n",
        "    # Training loop with progress bar\n",
        "    pbar = tqdm(range(episodes), desc=\"Training Episodes\")\n",
        "    for episode in pbar:\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            if len(agent.memory) >= agent.batch_size:\n",
        "                agent.replay()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            episode_steps += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'Reward': f'{total_reward:.2f}',\n",
        "            'Epsilon': f'{agent.epsilon:.2f}'\n",
        "        })\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            torch.save(agent.model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "    print(f\"Average time per episode: {training_time/episodes:.2f} seconds\")\n",
        "    print(f\"Best reward achieved: {best_reward:.2f}\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Estimate runtime\n",
        "    sample_size = 10000  # Reduced dataset size\n",
        "    episodes = 50        # Reduced number of episodes\n",
        "\n",
        "    print(\"Estimated runtime:\")\n",
        "    print(f\"- Data preparation: ~5-10 seconds\")\n",
        "    print(f\"- Training ({episodes} episodes): ~{episodes * 2}-{episodes * 3} seconds\")\n",
        "    print(f\"- Total estimated time: ~{episodes * 3 + 10} seconds\")\n",
        "    print(\"\\nStarting training pipeline...\")\n",
        "\n",
        "    trained_agent = train_model(episodes=episodes, sample_size=sample_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        # Enhanced architecture with larger layers\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size)\n",
        "        )\n",
        "\n",
        "        # Initialize weights for better training\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            module.bias.data.fill_(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class DataCenterEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.reset()\n",
        "        self.action_space = 5\n",
        "        self.prev_energy = None\n",
        "        self.energy_threshold = 0.7  # Threshold for high energy consumption\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.total_energy = 0\n",
        "        self.temperature_violations = 0\n",
        "        self.prev_energy = None\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "        energy_change = 0 if self.prev_energy is None else current_data['meter_reading'] - self.prev_energy\n",
        "\n",
        "        return np.array([\n",
        "            current_data['meter_reading'],\n",
        "            current_data['temperature'],\n",
        "            current_data['humidity'],\n",
        "            current_data['hour'],\n",
        "            current_data['day'],\n",
        "            self.total_energy,\n",
        "            energy_change,  # Added energy change as a feature\n",
        "            self.temperature_violations\n",
        "        ])\n",
        "\n",
        "    def step(self, action):\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "\n",
        "        # More granular cooling adjustment\n",
        "        cooling_adjustment = (action - 2) * 0.1\n",
        "\n",
        "        # Calculate energy impact\n",
        "        base_energy = current_data['meter_reading']\n",
        "        new_energy = base_energy * (1 + cooling_adjustment)\n",
        "\n",
        "        # Enhanced reward calculation\n",
        "        reward = 0\n",
        "\n",
        "        # Energy efficiency reward\n",
        "        energy_savings = base_energy - new_energy\n",
        "        reward += energy_savings * 20  # Increased weight for energy savings\n",
        "\n",
        "        # Temperature management reward\n",
        "        temp = current_data['temperature']\n",
        "        optimal_temp_range = (0.4, 0.6)  # Normalized temperature range\n",
        "\n",
        "        if optimal_temp_range[0] <= temp <= optimal_temp_range[1]:\n",
        "            reward += 10  # Reward for maintaining optimal temperature\n",
        "        elif temp > 0.7 and action < 2:\n",
        "            reward -= 100  # Increased penalty for temperature violations\n",
        "            self.temperature_violations += 1\n",
        "        elif temp < 0.3 and action > 2:\n",
        "            reward -= 50  # Penalty for overcooling\n",
        "\n",
        "        # Stability reward\n",
        "        if self.prev_energy is not None:\n",
        "            energy_change = abs(new_energy - self.prev_energy)\n",
        "            reward -= energy_change * 5  # Penalty for large energy fluctuations\n",
        "\n",
        "        # Update state\n",
        "        self.prev_energy = new_energy\n",
        "        self.total_energy += new_energy\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Check if episode is done\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        return self._get_state(), reward, done\n",
        "\n",
        "class DRLAgent:\n",
        "    def __init__(self, state_size, action_size, batch_size=64):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=10000)  # Increased memory size\n",
        "        self.gamma = 0.99    # Increased discount factor\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.0005  # Reduced learning rate for stability\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = DQN(state_size, action_size).to(self.device)\n",
        "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
        "        self.update_target_model()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            return np.argmax(self.model(state).cpu().numpy())\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = torch.FloatTensor([i[0] for i in minibatch]).to(self.device)\n",
        "        actions = torch.LongTensor([i[1] for i in minibatch]).to(self.device)\n",
        "        rewards = torch.FloatTensor([i[2] for i in minibatch]).to(self.device)\n",
        "        next_states = torch.FloatTensor([i[3] for i in minibatch]).to(self.device)\n",
        "        dones = torch.FloatTensor([i[4] for i in minibatch]).to(self.device)\n",
        "\n",
        "        # Double DQN implementation\n",
        "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
        "        next_actions = self.model(next_states).max(1)[1].unsqueeze(1)\n",
        "        next_q = self.target_model(next_states).gather(1, next_actions).squeeze(1)\n",
        "        target_q = rewards + (self.gamma * next_q * (1 - dones))\n",
        "\n",
        "        # Huber loss for better stability\n",
        "        loss = nn.SmoothL1Loss()(current_q.squeeze(), target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "def prepare_data(sample_size=10000):\n",
        "    print(\"Loading and preparing data...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_df = pd.read_csv('train.csv', nrows=sample_size)\n",
        "\n",
        "    train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
        "    train_df['hour'] = train_df['timestamp'].dt.hour\n",
        "    train_df['day'] = train_df['timestamp'].dt.dayofweek\n",
        "\n",
        "    train_df = train_df[train_df['meter'] == 0].reset_index(drop=True)\n",
        "\n",
        "    # More realistic temperature and humidity patterns\n",
        "    train_df['temperature'] = 20 + 5 * np.sin(train_df['hour'] * 2 * np.pi / 24) + \\\n",
        "                             2 * np.sin(train_df['day'] * 2 * np.pi / 7)\n",
        "    train_df['humidity'] = 50 + 20 * np.sin(train_df['hour'] * 2 * np.pi / 24 + np.pi/4) + \\\n",
        "                          10 * np.sin(train_df['day'] * 2 * np.pi / 7)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_columns = ['meter_reading', 'temperature', 'humidity', 'hour', 'day']\n",
        "    train_df[numerical_columns] = scaler.fit_transform(train_df[numerical_columns])\n",
        "\n",
        "    print(f\"Data preparation completed in {time.time() - start_time:.2f} seconds\")\n",
        "    print(f\"Final dataset shape: {train_df.shape}\")\n",
        "\n",
        "    return train_df\n",
        "\n",
        "def train_model(episodes=50, sample_size=10000):\n",
        "    print(f\"\\nDevice being used: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "    data = prepare_data(sample_size)\n",
        "\n",
        "    env = DataCenterEnvironment(data)\n",
        "    state_size = 8  # Updated for new state space\n",
        "    action_size = 5\n",
        "    agent = DRLAgent(state_size, action_size)\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    start_time = time.time()\n",
        "    best_reward = float('-inf')\n",
        "\n",
        "    pbar = tqdm(range(episodes), desc=\"Training Episodes\")\n",
        "    for episode in pbar:\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            if len(agent.memory) >= agent.batch_size:\n",
        "                agent.replay()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                # Update target network every episode\n",
        "                agent.update_target_model()\n",
        "                break\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'Reward': f'{total_reward:.2f}',\n",
        "            'Epsilon': f'{agent.epsilon:.2f}'\n",
        "        })\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            torch.save(agent.model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "    print(f\"Average time per episode: {training_time/episodes:.2f} seconds\")\n",
        "    print(f\"Best reward achieved: {best_reward:.2f}\")\n",
        "\n",
        "    return agent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trained_agent = train_model(episodes=50, sample_size=10000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs4ETCIpLy_7",
        "outputId": "bc9d70c3-870b-4b12-f692-0f092c5bb62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Device being used: CUDA\n",
            "Loading and preparing data...\n",
            "Data preparation completed in 0.03 seconds\n",
            "Final dataset shape: (6197, 8)\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes: 100%|██████████| 50/50 [26:17<00:00, 31.54s/it, Reward=11874.18, Epsilon=0.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed in 1577.16 seconds\n",
            "Average time per episode: 31.54 seconds\n",
            "Best reward achieved: 12378.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NEXT**"
      ],
      "metadata": {
        "id": "0BLYESPVtQZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.distributions import Normal\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# PPO Network Architecture\n",
        "class PPONetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(PPONetwork, self).__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Actor head (policy)\n",
        "        self.actor_mean = nn.Linear(64, action_size)\n",
        "        self.actor_std = nn.Parameter(torch.ones(action_size) * 0.1)\n",
        "\n",
        "        # Critic head (value)\n",
        "        self.critic = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        features = self.shared(state)\n",
        "        return self.actor_mean(features), self.actor_std, self.critic(features)\n",
        "\n",
        "class EnergyEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.total_energy = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        return np.array([\n",
        "            row['meter_reading'],\n",
        "            row['temperature'],\n",
        "            row['hour'],\n",
        "            row['day'],\n",
        "            self.total_energy\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "\n",
        "        # Apply action to adjust energy consumption\n",
        "        adjustment = float(action[0])  # Convert to float\n",
        "        energy_consumption = row['meter_reading'] * (1 + adjustment * 0.1)\n",
        "\n",
        "        # Calculate reward\n",
        "        baseline_energy = row['meter_reading']\n",
        "        energy_saving = baseline_energy - energy_consumption\n",
        "        temperature = row['temperature']\n",
        "\n",
        "        # Reward function\n",
        "        reward = energy_saving * 10  # Base reward for energy saving\n",
        "\n",
        "        # Temperature comfort penalty\n",
        "        if not (0.4 <= temperature <= 0.6):\n",
        "            reward -= 5\n",
        "\n",
        "        self.total_energy += energy_consumption\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        next_state = self._get_state() if not done else self.reset()\n",
        "        return next_state, reward, done\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_size, action_size, device):\n",
        "        self.device = device\n",
        "        self.network = PPONetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=3e-4)\n",
        "\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.2\n",
        "        self.c1 = 1  # Value loss coefficient\n",
        "        self.c2 = 0.01  # Entropy coefficient\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            mean, std, value = self.network(state)\n",
        "            dist = Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.values.append(value)\n",
        "        self.log_probs.append(log_prob)\n",
        "\n",
        "        return action.cpu().numpy()\n",
        "\n",
        "    def update(self):\n",
        "        states = torch.cat(self.states)\n",
        "        actions = torch.cat(self.actions)\n",
        "        old_log_probs = torch.cat(self.log_probs)\n",
        "\n",
        "        # Calculate returns and advantages\n",
        "        returns = []\n",
        "        R = 0\n",
        "        for r in reversed(self.rewards):\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.FloatTensor(returns).to(self.device)\n",
        "\n",
        "        # Normalize returns\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # PPO update\n",
        "        for _ in range(5):  # 5 epochs of optimization\n",
        "            mean, std, values = self.network(states)\n",
        "            dist = Normal(mean, std)\n",
        "            new_log_probs = dist.log_prob(actions).sum(dim=-1)\n",
        "\n",
        "            # Calculate ratio and surrogate loss\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "            surrogate1 = ratio * returns\n",
        "            surrogate2 = torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon) * returns\n",
        "\n",
        "            # Calculate losses\n",
        "            actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
        "            critic_loss = self.c1 * nn.MSELoss()(values.squeeze(), returns)\n",
        "            entropy_loss = -self.c2 * dist.entropy().mean()\n",
        "\n",
        "            total_loss = actor_loss + critic_loss + entropy_loss\n",
        "\n",
        "            # Update network\n",
        "            self.optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Clear memory\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "\n",
        "def prepare_data(sample_size=10000):\n",
        "    print(\"Loading and preparing data...\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv('train.csv', nrows=sample_size)\n",
        "\n",
        "    # Extract time features\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['hour'] = df['timestamp'].dt.hour / 24.0  # Normalize to [0,1]\n",
        "    df['day'] = df['timestamp'].dt.dayofweek / 6.0  # Normalize to [0,1]\n",
        "\n",
        "    # Add synthetic temperature (normalized)\n",
        "    df['temperature'] = np.sin(df['hour'] * 2 * np.pi) * 0.5 + 0.5\n",
        "\n",
        "    # Normalize meter readings\n",
        "    scaler = MinMaxScaler()\n",
        "    df['meter_reading'] = scaler.fit_transform(df[['meter_reading']])\n",
        "\n",
        "    # Select relevant columns\n",
        "    df = df[['meter_reading', 'temperature', 'hour', 'day']]\n",
        "\n",
        "    print(f\"Data prepared with shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def train_ppo(episodes=50, sample_size=10000):\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Prepare environment\n",
        "    data = prepare_data(sample_size)\n",
        "    env = EnergyEnvironment(data)\n",
        "\n",
        "    # Initialize agent\n",
        "    state_size = 5  # [meter_reading, temperature, hour, day, total_energy]\n",
        "    action_size = 1  # Single continuous action for energy adjustment\n",
        "    agent = PPOAgent(state_size, action_size, device)\n",
        "\n",
        "    # Training loop\n",
        "    best_reward = float('-inf')\n",
        "    pbar = tqdm(range(episodes))\n",
        "\n",
        "    for episode in pbar:\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Select and perform action\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Store reward\n",
        "            agent.rewards.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Update agent\n",
        "        agent.update()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'reward': f'{episode_reward:.2f}'})\n",
        "\n",
        "        # Save best model\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            torch.save(agent.network.state_dict(), 'best_ppo_model.pth')\n",
        "\n",
        "    print(f\"\\nTraining completed. Best reward: {best_reward:.2f}\")\n",
        "    return agent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train model\n",
        "    agent = train_ppo(episodes=50, sample_size=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzVcwYYMe-jd",
        "outputId": "b299c7f9-260b-4c96-d629-bde1cfde04dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading and preparing data...\n",
            "Data prepared with shape: (10000, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-13-0eb1eb969a9c>:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  adjustment = float(action[0])  # Convert to float\n",
            "100%|██████████| 50/50 [09:16<00:00, 11.13s/it, reward=-38488.97]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed. Best reward: -38488.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.distributions import Normal\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# PPO Network Architecture\n",
        "class PPONetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(PPONetwork, self).__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),  # Increased hidden size\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),  # Increased hidden size\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Actor head (policy)\n",
        "        self.actor_mean = nn.Linear(128, action_size)\n",
        "        self.actor_std = nn.Parameter(torch.ones(action_size) * 0.1)\n",
        "\n",
        "        # Critic head (value)\n",
        "        self.critic = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        features = self.shared(state)\n",
        "        return self.actor_mean(features), self.actor_std, self.critic(features)\n",
        "\n",
        "class EnergyEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.total_energy = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        return np.array([\n",
        "            row['meter_reading'],\n",
        "            row['temperature'],\n",
        "            row['hour'],\n",
        "            row['day'],\n",
        "            self.total_energy\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "\n",
        "        # Clip action to avoid large adjustments\n",
        "        adjustment = np.clip(float(action[0]), -0.5, 0.5)\n",
        "        energy_consumption = row['meter_reading'] * (1 + adjustment * 0.1)\n",
        "\n",
        "        # Calculate reward\n",
        "        baseline_energy = row['meter_reading']\n",
        "        energy_saving = baseline_energy - energy_consumption\n",
        "        temperature = row['temperature']\n",
        "\n",
        "        # Adjust reward function to provide a more balanced incentive\n",
        "        reward = energy_saving * 10  # Base reward for energy saving\n",
        "\n",
        "        # Penalize more heavily for extreme temperature discomfort\n",
        "        if not (0.4 <= temperature <= 0.6):\n",
        "            reward -= abs(temperature - 0.5) * 10  # Gradual penalty based on deviation from comfort zone\n",
        "\n",
        "        self.total_energy += energy_consumption\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        next_state = self._get_state() if not done else self.reset()\n",
        "        return next_state, reward, done\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_size, action_size, device):\n",
        "        self.device = device\n",
        "        self.network = PPONetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=1e-4)  # Lowered learning rate\n",
        "\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.2\n",
        "        self.c1 = 0.5  # Adjusted value loss coefficient\n",
        "        self.c2 = 0.01  # Entropy coefficient\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            mean, std, value = self.network(state)\n",
        "            dist = Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.values.append(value)\n",
        "        self.log_probs.append(log_prob)\n",
        "\n",
        "        return action.cpu().numpy()\n",
        "\n",
        "    def update(self):\n",
        "        states = torch.cat(self.states)\n",
        "        actions = torch.cat(self.actions)\n",
        "        old_log_probs = torch.cat(self.log_probs)\n",
        "\n",
        "        # Calculate returns and advantages\n",
        "        returns = []\n",
        "        R = 0\n",
        "        for r in reversed(self.rewards):\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.FloatTensor(returns).to(self.device)\n",
        "\n",
        "        # Normalize returns\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "        # PPO update\n",
        "        for _ in range(5):  # 5 epochs of optimization\n",
        "            mean, std, values = self.network(states)\n",
        "            dist = Normal(mean, std)\n",
        "            new_log_probs = dist.log_prob(actions).sum(dim=-1)\n",
        "\n",
        "            # Calculate ratio and surrogate loss\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "            surrogate1 = ratio * returns\n",
        "            surrogate2 = torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon) * returns\n",
        "\n",
        "            # Calculate losses\n",
        "            actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
        "            critic_loss = self.c1 * nn.MSELoss()(values.squeeze(), returns)\n",
        "            entropy_loss = -self.c2 * dist.entropy().mean()\n",
        "\n",
        "            total_loss = actor_loss + critic_loss + entropy_loss\n",
        "\n",
        "            # Update network\n",
        "            self.optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Clear memory\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.log_probs = []\n",
        "\n",
        "def prepare_data(sample_size=10000):\n",
        "    print(\"Loading and preparing data...\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv('train.csv', nrows=sample_size)\n",
        "\n",
        "    # Extract time features\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['hour'] = df['timestamp'].dt.hour / 24.0  # Normalize to [0,1]\n",
        "    df['day'] = df['timestamp'].dt.dayofweek / 6.0  # Normalize to [0,1]\n",
        "\n",
        "    # Add synthetic temperature (normalized)\n",
        "    df['temperature'] = np.sin(df['hour'] * 2 * np.pi) * 0.5 + 0.5\n",
        "\n",
        "    # Normalize meter readings\n",
        "    scaler = MinMaxScaler()\n",
        "    df['meter_reading'] = scaler.fit_transform(df[['meter_reading']])\n",
        "\n",
        "    # Select relevant columns\n",
        "    df = df[['meter_reading', 'temperature', 'hour', 'day']]\n",
        "\n",
        "    print(f\"Data prepared with shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def train_ppo(episodes=50, sample_size=10000):\n",
        "    # Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Prepare environment\n",
        "    data = prepare_data(sample_size)\n",
        "    env = EnergyEnvironment(data)\n",
        "\n",
        "    # Initialize agent\n",
        "    state_size = 5  # [meter_reading, temperature, hour, day, total_energy]\n",
        "    action_size = 1  # Single continuous action for energy adjustment\n",
        "    agent = PPOAgent(state_size, action_size, device)\n",
        "\n",
        "    # Training loop\n",
        "    best_reward = float('-inf')\n",
        "    pbar = tqdm(range(episodes))\n",
        "\n",
        "    for episode in pbar:\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Select and perform action\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            # Store reward\n",
        "            agent.rewards.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Update agent\n",
        "        agent.update()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'reward': f'{episode_reward:.2f}'})\n",
        "\n",
        "        # Save best model\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            torch.save(agent.network.state_dict(), 'best_ppo_model.pth')\n",
        "\n",
        "    print(f\"\\nTraining completed. Best reward: {best_reward:.2f}\")\n",
        "    return agent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train model\n",
        "    agent = train_ppo(episodes=50, sample_size=10000)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFJk49gKfSkc",
        "outputId": "e20d46bb-723e-4e9f-ba95-b50a543cda83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading and preparing data...\n",
            "Data prepared with shape: (10000, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-1-478bc7a20df1>:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  adjustment = np.clip(float(action[0]), -0.5, 0.5)\n",
            "100%|██████████| 50/50 [09:28<00:00, 11.38s/it, reward=-20340.44]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed. Best reward: -20339.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DDPG**"
      ],
      "metadata": {
        "id": "yHgB-DgsufGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import random\n",
        "\n",
        "# Actor Network (Policy)\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size),\n",
        "            nn.Tanh()  # Tanh to output actions in a fixed range\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "\n",
        "# Critic Network (Q-Value)\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size + action_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)  # Output single Q-value\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        return self.network(x)\n",
        "\n",
        "class EnergyEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.total_energy = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        return np.array([\n",
        "            row['meter_reading'],\n",
        "            row['temperature'],\n",
        "            row['hour'],\n",
        "            row['day'],\n",
        "            self.total_energy\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "\n",
        "        adjustment = np.clip(float(action[0]), -0.5, 0.5)\n",
        "        energy_consumption = row['meter_reading'] * (1 + adjustment * 0.1)\n",
        "\n",
        "        baseline_energy = row['meter_reading']\n",
        "        energy_saving = baseline_energy - energy_consumption\n",
        "        temperature = row['temperature']\n",
        "\n",
        "        reward = energy_saving * 10\n",
        "        if not (0.4 <= temperature <= 0.6):\n",
        "            reward -= abs(temperature - 0.5) * 10\n",
        "\n",
        "        self.total_energy += energy_consumption\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        next_state = self._get_state() if not done else self.reset()\n",
        "        return next_state, reward, done\n",
        "\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_size, action_size, device):\n",
        "        self.device = device\n",
        "        self.actor = Actor(state_size, action_size).to(device)\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.target_actor = copy.deepcopy(self.actor)\n",
        "        self.target_critic = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.memory = []  # Replay buffer\n",
        "        self.memory_size = 10000\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005  # Soft update factor\n",
        "\n",
        "    def select_action(self, state, noise_scale=0.1):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        action += noise_scale * np.random.randn(action.shape[0])  # Add noise for exploration\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.memory_size:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample a batch from memory using random.sample\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
        "        actions = torch.FloatTensor(np.array(actions)).to(self.device)\n",
        "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
        "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # Critic update\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.target_actor(next_states)\n",
        "            target_Q = rewards + self.gamma * (1 - dones) * self.target_critic(next_states, next_actions)\n",
        "\n",
        "        current_Q = self.critic(states, actions)\n",
        "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Actor update\n",
        "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Soft update of target networks\n",
        "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "def prepare_data(sample_size=10000):\n",
        "    print(\"Loading and preparing data...\")\n",
        "\n",
        "    df = pd.read_csv('train.csv', nrows=sample_size)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['hour'] = df['timestamp'].dt.hour / 24.0\n",
        "    df['day'] = df['timestamp'].dt.dayofweek / 6.0\n",
        "    df['temperature'] = np.sin(df['hour'] * 2 * np.pi) * 0.5 + 0.5\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df['meter_reading'] = scaler.fit_transform(df[['meter_reading']])\n",
        "\n",
        "    df = df[['meter_reading', 'temperature', 'hour', 'day']]\n",
        "\n",
        "    print(f\"Data prepared with shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def train_ddpg(episodes=50, sample_size=10000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    data = prepare_data(sample_size)\n",
        "    env = EnergyEnvironment(data)\n",
        "\n",
        "    state_size = 5\n",
        "    action_size = 1\n",
        "    agent = DDPGAgent(state_size, action_size, device)\n",
        "\n",
        "    best_reward = float('-inf')\n",
        "    pbar = tqdm(range(episodes))\n",
        "\n",
        "    for episode in pbar:\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        agent.update()\n",
        "        pbar.set_postfix({'reward': f'{episode_reward:.2f}'})\n",
        "\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            torch.save(agent.actor.state_dict(), 'best_ddpg_actor.pth')\n",
        "            torch.save(agent.critic.state_dict(), 'best_ddpg_critic.pth')\n",
        "\n",
        "    print(f\"\\nTraining completed. Best reward: {best_reward:.2f}\")\n",
        "    return agent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = train_ddpg(episodes=50, sample_size=10000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTPCCBmI-xM_",
        "outputId": "a639e7d8-508d-4d2c-b41f-0e8f62d36572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading and preparing data...\n",
            "Data prepared with shape: (10000, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [05:21<00:00,  6.43s/it, reward=-20340.81]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed. Best reward: -20339.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import random\n",
        "\n",
        "# Actor Network (Policy)\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size),\n",
        "            nn.Tanh()  # Tanh to output actions in the range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "\n",
        "# Critic Network (Q-Value)\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size + action_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)  # Output single Q-value\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        return self.network(x)\n",
        "\n",
        "class EnergyEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.total_energy = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        return np.array([\n",
        "            row['meter_reading'],\n",
        "            row['temperature'],\n",
        "            row['hour'],\n",
        "            row['day'],\n",
        "            self.total_energy\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "\n",
        "        adjustment = np.clip(float(action[0]), -0.5, 0.5)\n",
        "        energy_consumption = row['meter_reading'] * (1 + adjustment * 0.1)\n",
        "\n",
        "        baseline_energy = row['meter_reading']\n",
        "        energy_saving = baseline_energy - energy_consumption\n",
        "        temperature = row['temperature']\n",
        "\n",
        "        # Reward based on energy savings\n",
        "        reward = energy_saving * 10  # Scale energy savings\n",
        "\n",
        "        # Reward for maintaining comfort temperature\n",
        "        if 0.4 <= temperature <= 0.6:\n",
        "            reward += 15  # More positive reward for staying within comfort range\n",
        "        else:\n",
        "            reward -= abs(temperature - 0.5) * 5  # Penalty scaled by deviation from comfort\n",
        "\n",
        "        # Penalize large actions to prevent extreme behaviors\n",
        "        if abs(adjustment) > 0.4:\n",
        "            reward -= 5\n",
        "\n",
        "        self.total_energy += energy_consumption\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        # Return next state, reward, and done flag\n",
        "        next_state = self._get_state() if not done else self.reset()\n",
        "        return next_state, reward, done\n",
        "\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_size, action_size, device):\n",
        "        self.device = device\n",
        "        self.actor = Actor(state_size, action_size).to(device)\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.target_actor = copy.deepcopy(self.actor)\n",
        "        self.target_critic = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        self.memory = []  # Replay buffer\n",
        "        self.memory_size = 10000\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005  # Soft update factor\n",
        "\n",
        "    def select_action(self, state, noise_scale=0.1):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        action += noise_scale * np.random.randn(action.shape[0])  # Add noise for exploration\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.memory_size:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample a batch from memory using random.sample\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
        "        actions = torch.FloatTensor(np.array(actions)).to(self.device)\n",
        "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
        "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(self.device)\n",
        "\n",
        "        # Critic update\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.target_actor(next_states)\n",
        "            target_Q = rewards + self.gamma * (1 - dones) * self.target_critic(next_states, next_actions)\n",
        "\n",
        "        current_Q = self.critic(states, actions)\n",
        "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Actor update\n",
        "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Soft update of target networks\n",
        "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "def prepare_data(sample_size=10000):\n",
        "    print(\"Loading and preparing data...\")\n",
        "\n",
        "    df = pd.read_csv('train.csv', nrows=sample_size)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['hour'] = df['timestamp'].dt.hour / 24.0\n",
        "    df['day'] = df['timestamp'].dt.dayofweek / 6.0\n",
        "    df['temperature'] = np.sin(df['hour'] * 2 * np.pi) * 0.5 + 0.5\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df['meter_reading'] = scaler.fit_transform(df[['meter_reading']])\n",
        "\n",
        "    df = df[['meter_reading', 'temperature', 'hour', 'day']]\n",
        "\n",
        "    print(f\"Data prepared with shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def train_ddpg(episodes=200, sample_size=10000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    data = prepare_data(sample_size)\n",
        "    env = EnergyEnvironment(data)\n",
        "\n",
        "    state_size = 5\n",
        "    action_size = 1\n",
        "    agent = DDPGAgent(state_size, action_size, device)\n",
        "\n",
        "    best_reward = float('-inf')\n",
        "    pbar = tqdm(range(episodes))\n",
        "\n",
        "    for episode in pbar:\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        agent.update()\n",
        "        pbar.set_postfix({'reward': f'{episode_reward:.2f}'})\n",
        "\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            torch.save(agent.actor.state_dict(), 'best_ddpg_actor.pth')\n",
        "            torch.save(agent.critic.state_dict(), 'best_ddpg_critic.pth')\n",
        "\n",
        "    print(f\"\\nTraining completed. Best reward: {best_reward:.2f}\")\n",
        "    return agent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent = train_ddpg(episodes=200, sample_size=10000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPuu0K2iGQHe",
        "outputId": "4cab5c64-fd80-4c0a-e540-d2fb5f207c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading and preparing data...\n",
            "Data prepared with shape: (10000, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [22:09<00:00,  6.65s/it, reward=2523.62]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed. Best reward: 24345.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TD3**"
      ],
      "metadata": {
        "id": "XSS67ohvutVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Actor Network (Policy) with Smaller Architecture\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "\n",
        "# Twin Critic Networks for TD3 with Smaller Architecture\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size + action_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        return self.network(x)\n",
        "\n",
        "# Energy Environment for managing energy optimization\n",
        "class EnergyEnvironment:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.total_energy = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        return np.array([\n",
        "            row['meter_reading'],\n",
        "            row['temperature'],\n",
        "            row['hour'],\n",
        "            row['day'],\n",
        "            self.total_energy\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "\n",
        "        # Clip action to avoid extreme adjustments\n",
        "        adjustment = np.clip(float(action[0]), -0.5, 0.5)\n",
        "        energy_consumption = row['meter_reading'] * (1 + adjustment * 0.1)\n",
        "\n",
        "        baseline_energy = row['meter_reading']\n",
        "        energy_saving = baseline_energy - energy_consumption\n",
        "        temperature = row['temperature']\n",
        "\n",
        "        # Reward based on energy savings and temperature comfort\n",
        "        reward = energy_saving * 10\n",
        "        if 0.4 <= temperature <= 0.6:\n",
        "            reward += 15\n",
        "        else:\n",
        "            reward -= abs(temperature - 0.5) * 5\n",
        "\n",
        "        if abs(adjustment) > 0.4:\n",
        "            reward -= 5\n",
        "\n",
        "        self.total_energy += energy_consumption\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        next_state = self._get_state() if not done else self.reset()\n",
        "        return next_state, reward, done\n",
        "\n",
        "class TD3Agent:\n",
        "    def __init__(self, state_size, action_size, device):\n",
        "        self.device = device\n",
        "        self.actor = Actor(state_size, action_size).to(device)\n",
        "        self.critic1 = Critic(state_size, action_size).to(device)\n",
        "        self.critic2 = Critic(state_size, action_size).to(device)\n",
        "\n",
        "        # Target networks\n",
        "        self.target_actor = copy.deepcopy(self.actor)\n",
        "        self.target_critic1 = copy.deepcopy(self.critic1)\n",
        "        self.target_critic2 = copy.deepcopy(self.critic2)\n",
        "\n",
        "        # Optimizers\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
        "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=1e-3)\n",
        "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=1e-3)\n",
        "\n",
        "        self.memory = []\n",
        "        self.memory_size = 5000\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "        self.policy_delay = 2\n",
        "\n",
        "    def select_action(self, state, noise_scale=0.1):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        action += noise_scale * np.random.randn(action.shape[0])\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.memory_size:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def update(self, step):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
        "        actions = torch.FloatTensor(np.array(actions)).to(self.device)\n",
        "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
        "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(1).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            noise = torch.clamp(torch.randn_like(actions) * 0.2, -0.5, 0.5)\n",
        "            next_action = torch.clamp(self.target_actor(next_states) + noise, -1, 1)\n",
        "            target_Q1 = self.target_critic1(next_states, next_action)\n",
        "            target_Q2 = self.target_critic2(next_states, next_action)\n",
        "            target_Q = rewards + self.gamma * (1 - dones) * torch.min(target_Q1, target_Q2)\n",
        "\n",
        "        current_Q1 = self.critic1(states, actions)\n",
        "        current_Q2 = self.critic2(states, actions)\n",
        "        critic1_loss = nn.MSELoss()(current_Q1, target_Q)\n",
        "        critic2_loss = nn.MSELoss()(current_Q2, target_Q)\n",
        "\n",
        "        self.critic1_optimizer.zero_grad()\n",
        "        critic1_loss.backward()\n",
        "        self.critic1_optimizer.step()\n",
        "\n",
        "        self.critic2_optimizer.zero_grad()\n",
        "        critic2_loss.backward()\n",
        "        self.critic2_optimizer.step()\n",
        "\n",
        "        if step % self.policy_delay == 0:\n",
        "            actor_loss = -self.critic1(states, self.actor(states)).mean()\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "            for target_param, param in zip(self.target_critic1.parameters(), self.critic1.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "            for target_param, param in zip(self.target_critic2.parameters(), self.critic2.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "def prepare_data(sample_size=5000):\n",
        "    df = pd.read_csv('train.csv', nrows=sample_size)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df['hour'] = df['timestamp'].dt.hour / 24.0\n",
        "    df['day'] = df['timestamp'].dt.dayofweek / 6.0\n",
        "    df['temperature'] = np.sin(df['hour'] * 2 * np.pi) * 0.5 + 0.5\n",
        "    scaler = MinMaxScaler()\n",
        "    df['meter_reading'] = scaler.fit_transform(df[['meter_reading']])\n",
        "    df = df[['meter_reading', 'temperature', 'hour', 'day']]\n",
        "    return df\n",
        "\n",
        "def train_td3(episodes=20, sample_size=5000):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    data = prepare_data(sample_size)\n",
        "    env = EnergyEnvironment(data)\n",
        "    state_size = 5\n",
        "    action_size = 1\n",
        "    agent = TD3Agent(state_size, action_size, device)\n",
        "    episode_rewards = []\n",
        "\n",
        "    pbar = tqdm(range(episodes), desc=\"Training 20 episodes\")\n",
        "\n",
        "    for episode in pbar:\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if step % 2 == 0:\n",
        "                agent.update(step)\n",
        "            step += 1\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "\n",
        "        # Save model at the end of each episode\n",
        "        torch.save(agent.actor.state_dict(), f'td3_actor_episode_{episode + 1}.pth')\n",
        "\n",
        "        pbar.set_postfix({'reward': f'{episode_reward:.2f}'})\n",
        "\n",
        "    # Plot rewards after 20 episodes\n",
        "    plt.plot(episode_rewards, label='Episode Reward')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('TD3 Training Rewards for 20 Episodes')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nTraining completed. Average reward: {np.mean(episode_rewards):.2f}\")\n",
        "    return agent, episode_rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent, episode_rewards = train_td3(episodes=20, sample_size=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "HUQCKnUGcv08",
        "outputId": "1f962770-e267-4f63-ffdf-0f2469eec767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training 20 episodes: 100%|██████████| 20/20 [07:44<00:00, 23.23s/it, reward=30220.30]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed. Average reward: 29234.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tMotSmWi_DU",
        "outputId": "57a88b82-330a-46c9-f44d-9d9e750064da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.4.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, markupsafe, h11, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, safehttpx, gradio-client, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.3 ffmpy-0.4.0 gradio-5.4.0 gradio-client-1.4.2 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.1 markupsafe-2.1.5 orjson-3.10.10 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.1 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.0 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the Actor model class\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "\n",
        "# Load the saved model\n",
        "def load_model(path):\n",
        "    state_size = 5  # Adjust based on your model input\n",
        "    action_size = 1  # Adjust based on your model output\n",
        "    model = Actor(state_size, action_size)\n",
        "    model.load_state_dict(torch.load(path, map_location=torch.device('cpu'), weights_only=True))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Initialize model\n",
        "model_path = \"best_td3_actor.pth\"  # Adjust to your saved model file path\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Global variables to store data for the graph and energy savings\n",
        "predicted_actions = []\n",
        "energy_savings = []\n",
        "cumulative_savings = 0\n",
        "\n",
        "# Function to predict action and calculate energy savings\n",
        "def predict_action(state_meter_reading, state_temperature, state_hour, state_day, state_total_energy):\n",
        "    global cumulative_savings, predicted_actions, energy_savings\n",
        "\n",
        "    # Prepare the state\n",
        "    state = np.array([state_meter_reading, state_temperature, state_hour, state_day, state_total_energy], dtype=np.float32)\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "    # Model prediction\n",
        "    with torch.no_grad():\n",
        "        action = model(state_tensor).cpu().numpy().flatten()[0]\n",
        "\n",
        "    # Calculate energy savings\n",
        "    baseline_energy = state_meter_reading\n",
        "    adjusted_energy = baseline_energy * (1 + action * 0.1)\n",
        "    energy_saved = baseline_energy - adjusted_energy\n",
        "    cumulative_savings += energy_saved\n",
        "\n",
        "    # Update lists for graphing\n",
        "    predicted_actions.append(action)\n",
        "    energy_savings.append(cumulative_savings)\n",
        "\n",
        "    # Generate plot with minimum value for x-axis and y-axis to ensure visibility\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(predicted_actions, label=\"Predicted Action\")\n",
        "    plt.plot(energy_savings, label=\"Cumulative Energy Saved\", linestyle=\"--\")\n",
        "    plt.xlabel(\"Predictions\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.ylim(min(energy_savings) - 5, max(energy_savings) + 5)  # Dynamically adjust y-axis for visibility\n",
        "    plt.title(\"Predicted Actions and Cumulative Energy Saved\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Return the plot and the energy saved information\n",
        "    return plt, f\"Cumulative Energy Saved: {cumulative_savings:.2f} units\"\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_action,\n",
        "    inputs=[\n",
        "        gr.Number(label=\"Meter Reading\"),\n",
        "        gr.Number(label=\"Temperature\"),\n",
        "        gr.Number(label=\"Hour\"),\n",
        "        gr.Number(label=\"Day\"),\n",
        "        gr.Number(label=\"Total Energy\")\n",
        "    ],\n",
        "    outputs=[gr.Plot(label=\"Prediction Plot\"), gr.Textbox(label=\"Energy Saved\")],\n",
        "    title=\"Energy Optimization Model with Graph\",\n",
        "    description=\"Enter state values to predict the action using the trained model and see cumulative energy savings.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "0FLmFJplnADV",
        "outputId": "56538611-0d21-45a3-d79c-b32e4f6aac09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7499fad4d8a15abba1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7499fad4d8a15abba1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8KPlGFqoeMY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}